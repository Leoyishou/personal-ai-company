#!/usr/bin/env python3
"""
WebFetchPlus - æ™ºèƒ½ç½‘é¡µå†…å®¹æŠ“å–å·¥å…·
æ ¹æ®URLç±»å‹è‡ªåŠ¨é€‰æ‹©æœ€ä½³æŠ“å–ç­–ç•¥
"""

import re
import json
import os
import sys
from typing import Dict, Any, Optional, List, Union
from dataclasses import dataclass
from enum import Enum
from urllib.parse import urlparse
import requests
from datetime import datetime

# ==================== é…ç½®åŠ è½½ ====================
class FetchStrategy(Enum):
    """æŠ“å–ç­–ç•¥æšä¸¾"""
    TWITTER_API = "twitter_api"       # Twitter/Xä¸“ç”¨API
    FIRECRAWL = "firecrawl"           # é€šç”¨ç½‘é¡µæŠ“å–
    DOUYIN = "douyin"                 # æŠ–éŸ³ä¸‹è½½
    BILIBILI = "bilibili"             # Bç«™ä¸‹è½½
    XIAOHONGSHU = "xiaohongshu"       # å°çº¢ä¹¦
    YOUTUBE = "youtube"               # YouTube
    ZHIHU = "zhihu"                   # çŸ¥ä¹
    GENERIC = "generic"               # é€šç”¨HTTPæŠ“å–

@dataclass
class FetchConfig:
    """æŠ“å–é…ç½®"""
    twitter_api_key: str = ""
    firecrawl_api_key: str = ""
    proxy: Optional[str] = None
    timeout: int = 30
    max_retries: int = 3
    save_path: str = "~/webfetch_downloads"
    debug: bool = False

class URLDetector:
    """URLç±»å‹æ£€æµ‹å™¨"""

    PATTERNS = {
        FetchStrategy.TWITTER_API: [
            r'(?:twitter|x)\.com/[\w]+/status/\d+',
            r'(?:twitter|x)\.com/[\w]+$',
            r'(?:twitter|x)\.com/[\w]+/.*'
        ],
        FetchStrategy.DOUYIN: [
            r'douyin\.com/',
            r'iesdouyin\.com/',
            r'v\.douyin\.com/'
        ],
        FetchStrategy.BILIBILI: [
            r'bilibili\.com/video/',
            r'b23\.tv/',
            r'bilibili\.com/bangumi/'
        ],
        FetchStrategy.XIAOHONGSHU: [
            r'xiaohongshu\.com/',
            r'xhslink\.com/'
        ],
        FetchStrategy.YOUTUBE: [
            r'youtube\.com/watch',
            r'youtu\.be/',
            r'youtube\.com/shorts/'
        ],
        FetchStrategy.ZHIHU: [
            r'zhihu\.com/question/\d+',      # é—®é¢˜é¡µ
            r'zhihu\.com/answer/\d+',        # å›ç­”ç›´é“¾
            r'zhuanlan\.zhihu\.com/p/\d+',   # ä¸“æ æ–‡ç« 
            r'zhihu\.com/p/\d+',             # æ–°ç‰ˆæ–‡ç« 
            r'zhihu\.com/people/[\w-]+',     # ç”¨æˆ·ä¸»é¡µ
            r'zhihu\.com/column/[\w-]+'      # ä¸“æ 
        ]
    }

    @classmethod
    def detect(cls, url: str) -> FetchStrategy:
        """æ£€æµ‹URLç±»å‹å¹¶è¿”å›å¯¹åº”ç­–ç•¥"""
        for strategy, patterns in cls.PATTERNS.items():
            for pattern in patterns:
                if re.search(pattern, url, re.IGNORECASE):
                    return strategy
        return FetchStrategy.GENERIC

# ==================== Twitter API æŠ“å–å™¨ ====================
class TwitterFetcher:
    """ä½¿ç”¨ docs.twitterapi.io æŠ“å–Twitter/Xå†…å®¹"""

    BASE_URL = "https://api.twitterapi.io"

    def __init__(self, api_key: str):
        self.api_key = api_key
        self.headers = {
            "X-API-Key": api_key,
            "Content-Type": "application/json"
        }

    def fetch_tweet(self, url: str) -> Dict[str, Any]:
        """è·å–æ¨æ–‡è¯¦æƒ…"""
        # ä»URLæå–tweet_id
        tweet_id = self._extract_tweet_id(url)
        if not tweet_id:
            return {"error": "æ— æ³•ä»URLæå–æ¨æ–‡ID"}

        # ä½¿ç”¨æ­£ç¡®çš„APIç«¯ç‚¹æ ¼å¼
        endpoint = f"{self.BASE_URL}/twitter/tweets"
        params = {
            "tweet_ids": tweet_id  # APIæ¥å—tweet_idså‚æ•°
        }

        try:
            response = requests.get(endpoint, headers=self.headers, params=params)
            response.raise_for_status()
            return self._parse_tweet_response(response.json())
        except Exception as e:
            return {"error": f"è·å–æ¨æ–‡å¤±è´¥: {str(e)}"}

    def fetch_user(self, url: str) -> Dict[str, Any]:
        """è·å–ç”¨æˆ·ä¿¡æ¯"""
        username = self._extract_username(url)
        if not username:
            return {"error": "æ— æ³•ä»URLæå–ç”¨æˆ·å"}

        # ä½¿ç”¨æ­£ç¡®çš„APIç«¯ç‚¹æ ¼å¼
        endpoint = f"{self.BASE_URL}/twitter/users"
        params = {
            "usernames": username  # APIæ¥å—usernameså‚æ•°
        }

        try:
            response = requests.get(endpoint, headers=self.headers, params=params)
            response.raise_for_status()
            return self._parse_user_response(response.json())
        except Exception as e:
            return {"error": f"è·å–ç”¨æˆ·ä¿¡æ¯å¤±è´¥: {str(e)}"}

    def fetch_timeline(self, username: str, max_results: int = 10) -> Dict[str, Any]:
        """è·å–ç”¨æˆ·æ—¶é—´çº¿"""
        # é¦–å…ˆè·å–ç”¨æˆ·ID
        user_data = self.fetch_user(f"https://x.com/{username}")
        if "error" in user_data:
            return user_data

        user_id = user_data.get("id")
        if not user_id:
            return {"error": "æ— æ³•è·å–ç”¨æˆ·ID"}

        endpoint = f"{self.BASE_URL}/2/users/{user_id}/tweets"
        params = {
            "max_results": max_results,
            "tweet.fields": "created_at,text,public_metrics,attachments",
            "media.fields": "url,preview_image_url,type"
        }

        try:
            response = requests.get(endpoint, headers=self.headers, params=params)
            response.raise_for_status()
            return self._parse_timeline_response(response.json())
        except Exception as e:
            return {"error": f"è·å–æ—¶é—´çº¿å¤±è´¥: {str(e)}"}

    def search_tweets(self, query: str, max_results: int = 10) -> Dict[str, Any]:
        """æœç´¢æ¨æ–‡"""
        # TwitterAPI.io çš„æœç´¢ç«¯ç‚¹
        endpoint = f"{self.BASE_URL}/twitter/search"
        params = {
            "query": query,
            "limit": min(max_results, 100)  # APIé™åˆ¶
        }

        try:
            response = requests.get(endpoint, headers=self.headers, params=params)
            response.raise_for_status()
            return self._parse_search_response(response.json())
        except Exception as e:
            return {"error": f"æœç´¢å¤±è´¥: {str(e)}"}

    def _extract_tweet_id(self, url: str) -> Optional[str]:
        """ä»URLæå–æ¨æ–‡ID"""
        match = re.search(r'/status/(\d+)', url)
        return match.group(1) if match else None

    def _extract_username(self, url: str) -> Optional[str]:
        """ä»URLæå–ç”¨æˆ·å"""
        match = re.search(r'(?:twitter|x)\.com/(@?[\w]+)', url)
        if match:
            username = match.group(1)
            return username.lstrip('@')
        return None

    def _parse_tweet_response(self, data: Dict) -> Dict[str, Any]:
        """è§£ææ¨æ–‡å“åº” - é€‚é…TwitterAPI.ioçš„å“åº”æ ¼å¼"""
        # TwitterAPI.io è¿”å›æ ¼å¼: {"tweets": [{tweet_object}]}
        if "tweets" in data and isinstance(data["tweets"], list) and len(data["tweets"]) > 0:
            tweet = data["tweets"][0]
        elif isinstance(data, list) and len(data) > 0:
            tweet = data[0]
        else:
            return {"error": "æœªæ‰¾åˆ°æ¨æ–‡æ•°æ®"}

        # æå–æ ¸å¿ƒä¿¡æ¯ - åŸºäºå®é™…çš„TwitterAPI.ioå“åº”æ ¼å¼
        author_info = tweet.get("author", {})

        return {
            "id": tweet.get("id"),
            "text": tweet.get("text"),
            "created_at": tweet.get("createdAt"),
            "author": {
                "id": author_info.get("id"),
                "name": author_info.get("name"),
                "username": author_info.get("userName"),
                "verified": author_info.get("isVerified", False),
                "blue_verified": author_info.get("isBlueVerified", False),
                "profile_picture": author_info.get("profilePicture"),
                "description": author_info.get("description"),
                "followers": author_info.get("followers", 0),
                "following": author_info.get("following", 0)
            },
            "metrics": {
                "like_count": tweet.get("likeCount", 0),
                "retweet_count": tweet.get("retweetCount", 0),
                "reply_count": tweet.get("replyCount", 0),
                "quote_count": tweet.get("quoteCount", 0),
                "view_count": tweet.get("viewCount", 0),
                "bookmark_count": tweet.get("bookmarkCount", 0)
            },
            "media": tweet.get("media", []),
            "url": tweet.get("url"),
            "is_reply": tweet.get("isReply", False),
            "is_pinned": tweet.get("isPinned", False),
            "lang": tweet.get("lang")
        }

    def _parse_user_response(self, data: Dict) -> Dict[str, Any]:
        """è§£æç”¨æˆ·å“åº”"""
        if "data" not in data:
            return {"error": "å“åº”ä¸­æ²¡æœ‰æ•°æ®"}

        user = data["data"]
        return {
            "id": user.get("id"),
            "name": user.get("name"),
            "username": user.get("username"),
            "description": user.get("description"),
            "created_at": user.get("created_at"),
            "verified": user.get("verified"),
            "metrics": user.get("public_metrics"),
            "profile_image": user.get("profile_image_url"),
            "url": f"https://x.com/{user.get('username')}"
        }

    def _parse_timeline_response(self, data: Dict) -> Dict[str, Any]:
        """è§£ææ—¶é—´çº¿å“åº”"""
        tweets = data.get("data", [])
        return {
            "tweets": [
                {
                    "id": tweet.get("id"),
                    "text": tweet.get("text"),
                    "created_at": tweet.get("created_at"),
                    "metrics": tweet.get("public_metrics")
                }
                for tweet in tweets
            ],
            "count": len(tweets)
        }

    def _parse_search_response(self, data: Dict) -> Dict[str, Any]:
        """è§£ææœç´¢å“åº”"""
        tweets = data.get("data", [])
        includes = data.get("includes", {})
        users = {user["id"]: user for user in includes.get("users", [])}

        results = []
        for tweet in tweets:
            author_id = tweet.get("author_id")
            author = users.get(author_id, {})

            results.append({
                "id": tweet.get("id"),
                "text": tweet.get("text"),
                "created_at": tweet.get("created_at"),
                "author": {
                    "name": author.get("name"),
                    "username": author.get("username"),
                    "verified": author.get("verified", False)
                },
                "metrics": tweet.get("public_metrics")
            })

        return {
            "results": results,
            "count": len(results)
        }

# ==================== é€šç”¨æŠ“å–å™¨åŸºç±» ====================
class BaseFetcher:
    """æŠ“å–å™¨åŸºç±»"""

    def fetch(self, url: str, **kwargs) -> Dict[str, Any]:
        """æŠ“å–å†…å®¹"""
        raise NotImplementedError

    def download(self, url: str, save_path: str = None) -> Dict[str, Any]:
        """ä¸‹è½½å†…å®¹åˆ°æœ¬åœ°"""
        raise NotImplementedError

# ==================== ç¤¾äº¤åª’ä½“æŠ“å–å™¨ ====================
class SocialMediaFetcher(BaseFetcher):
    """ç¤¾äº¤åª’ä½“å†…å®¹æŠ“å–å™¨"""

    def __init__(self):
        self.douyin_cookies = self._load_douyin_cookies()

    def _load_douyin_cookies(self):
        """åŠ è½½æŠ–éŸ³cookies"""
        cookie_path = os.path.expanduser("~/.claude/douyin_cookies.txt")
        if os.path.exists(cookie_path):
            with open(cookie_path) as f:
                return f.read().strip()
        return None

    def fetch_douyin(self, url: str) -> Dict[str, Any]:
        """æŠ“å–æŠ–éŸ³å†…å®¹"""
        # è¿™é‡Œè°ƒç”¨ç°æœ‰çš„social-media-downloadè„šæœ¬
        import subprocess

        script_path = os.path.expanduser("~/.claude/skills/social-media-download/scripts/social_download.py")

        try:
            result = subprocess.run(
                [sys.executable, script_path, url, "--json"],
                capture_output=True,
                text=True
            )

            if result.returncode == 0:
                return json.loads(result.stdout)
            else:
                return {"error": f"æŠ–éŸ³æŠ“å–å¤±è´¥: {result.stderr}"}
        except Exception as e:
            return {"error": f"è°ƒç”¨æŠ–éŸ³æŠ“å–è„šæœ¬å¤±è´¥: {str(e)}"}

    def fetch_bilibili(self, url: str) -> Dict[str, Any]:
        """æŠ“å–Bç«™å†…å®¹"""
        # TODO: å®ç°Bç«™æŠ“å–
        return {"message": "Bç«™æŠ“å–åŠŸèƒ½å¾…å®ç°"}

    def fetch_xiaohongshu(self, url: str) -> Dict[str, Any]:
        """æŠ“å–å°çº¢ä¹¦å†…å®¹"""
        # TODO: å®ç°å°çº¢ä¹¦æŠ“å–
        return {"message": "å°çº¢ä¹¦æŠ“å–åŠŸèƒ½å¾…å®ç°"}

# ==================== çŸ¥ä¹æŠ“å–å™¨ ====================
class ZhihuFetcher(BaseFetcher):
    """çŸ¥ä¹å†…å®¹æŠ“å–å™¨"""

    HEADERS = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
        "Accept-Encoding": "gzip, deflate, br",
        "Connection": "keep-alive",
        "Referer": "https://www.zhihu.com/",
    }

    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update(self.HEADERS)

    def fetch(self, url: str, **kwargs) -> Dict[str, Any]:
        """æ ¹æ®URLç±»å‹è‡ªåŠ¨åˆ†å‘"""
        url_type = self._detect_url_type(url)

        if url_type == "question":
            return self.fetch_question(url)
        elif url_type == "answer":
            return self.fetch_answer(url)
        elif url_type == "article":
            return self.fetch_article(url)
        elif url_type == "people":
            return self.fetch_user(url)
        elif url_type == "column":
            return self.fetch_column(url)
        else:
            return {"error": f"æœªçŸ¥çš„çŸ¥ä¹URLç±»å‹: {url}"}

    def _detect_url_type(self, url: str) -> str:
        """æ£€æµ‹çŸ¥ä¹URLç±»å‹"""
        if "/question/" in url and "/answer/" in url:
            return "answer"
        elif "/question/" in url:
            return "question"
        elif "/answer/" in url:
            return "answer"
        elif "/p/" in url or "zhuanlan.zhihu.com" in url:
            return "article"
        elif "/people/" in url:
            return "people"
        elif "/column/" in url:
            return "column"
        return "unknown"

    def _extract_id(self, url: str, pattern: str) -> Optional[str]:
        """ä»URLæå–ID"""
        match = re.search(pattern, url)
        return match.group(1) if match else None

    def fetch_question(self, url: str) -> Dict[str, Any]:
        """æŠ“å–çŸ¥ä¹é—®é¢˜"""
        question_id = self._extract_id(url, r'/question/(\d+)')
        if not question_id:
            return {"error": "æ— æ³•æå–é—®é¢˜ID"}

        # ä½¿ç”¨çŸ¥ä¹APIè·å–é—®é¢˜ä¿¡æ¯
        api_url = f"https://www.zhihu.com/api/v4/questions/{question_id}"
        params = {"include": "data[*].answer_count,follower_count,visit_count,comment_count"}

        try:
            response = self.session.get(api_url, params=params, timeout=15)
            response.raise_for_status()
            data = response.json()

            # è·å–çƒ­é—¨å›ç­”
            answers_url = f"https://www.zhihu.com/api/v4/questions/{question_id}/answers"
            answers_params = {
                "include": "data[*].content,author.name,author.headline,voteup_count,comment_count,created_time",
                "limit": 5,
                "sort_by": "default"
            }
            answers_resp = self.session.get(answers_url, params=answers_params, timeout=15)
            answers_data = answers_resp.json() if answers_resp.status_code == 200 else {}

            return {
                "type": "question",
                "id": question_id,
                "title": data.get("title"),
                "detail": self._clean_html(data.get("detail", "")),
                "created": data.get("created"),
                "updated": data.get("updated_time"),
                "answer_count": data.get("answer_count", 0),
                "follower_count": data.get("follower_count", 0),
                "visit_count": data.get("visit_count", 0),
                "topics": [t.get("name") for t in data.get("topics", [])],
                "url": f"https://www.zhihu.com/question/{question_id}",
                "top_answers": self._parse_answers(answers_data.get("data", []))
            }
        except Exception as e:
            return {"error": f"è·å–é—®é¢˜å¤±è´¥: {str(e)}"}

    def fetch_answer(self, url: str) -> Dict[str, Any]:
        """æŠ“å–çŸ¥ä¹å›ç­”"""
        # æ”¯æŒä¸¤ç§URLæ ¼å¼
        answer_id = self._extract_id(url, r'/answer/(\d+)')
        if not answer_id:
            return {"error": "æ— æ³•æå–å›ç­”ID"}

        api_url = f"https://www.zhihu.com/api/v4/answers/{answer_id}"
        params = {"include": "data[*].content,author.name,author.headline,author.avatar_url,voteup_count,comment_count,created_time,updated_time,question.title"}

        try:
            response = self.session.get(api_url, params=params, timeout=15)
            response.raise_for_status()
            data = response.json()

            author = data.get("author", {})
            question = data.get("question", {})

            return {
                "type": "answer",
                "id": answer_id,
                "content": self._clean_html(data.get("content", "")),
                "excerpt": data.get("excerpt", ""),
                "voteup_count": data.get("voteup_count", 0),
                "comment_count": data.get("comment_count", 0),
                "created_time": data.get("created_time"),
                "updated_time": data.get("updated_time"),
                "author": {
                    "name": author.get("name"),
                    "headline": author.get("headline"),
                    "avatar": author.get("avatar_url"),
                    "url_token": author.get("url_token")
                },
                "question": {
                    "id": question.get("id"),
                    "title": question.get("title")
                },
                "url": url
            }
        except Exception as e:
            return {"error": f"è·å–å›ç­”å¤±è´¥: {str(e)}"}

    def fetch_article(self, url: str) -> Dict[str, Any]:
        """æŠ“å–çŸ¥ä¹ä¸“æ æ–‡ç« """
        article_id = self._extract_id(url, r'/p/(\d+)')
        if not article_id:
            return {"error": "æ— æ³•æå–æ–‡ç« ID"}

        api_url = f"https://www.zhihu.com/api/v4/articles/{article_id}"

        try:
            response = self.session.get(api_url, timeout=15)
            response.raise_for_status()
            data = response.json()

            author = data.get("author", {})

            return {
                "type": "article",
                "id": article_id,
                "title": data.get("title"),
                "content": self._clean_html(data.get("content", "")),
                "excerpt": data.get("excerpt", ""),
                "voteup_count": data.get("voteup_count", 0),
                "comment_count": data.get("comment_count", 0),
                "created": data.get("created"),
                "updated": data.get("updated"),
                "image_url": data.get("image_url"),
                "author": {
                    "name": author.get("name"),
                    "headline": author.get("headline"),
                    "avatar": author.get("avatar_url"),
                    "url_token": author.get("url_token")
                },
                "topics": [t.get("name") for t in data.get("topics", [])],
                "url": f"https://zhuanlan.zhihu.com/p/{article_id}"
            }
        except Exception as e:
            return {"error": f"è·å–æ–‡ç« å¤±è´¥: {str(e)}"}

    def fetch_user(self, url: str) -> Dict[str, Any]:
        """æŠ“å–çŸ¥ä¹ç”¨æˆ·ä¿¡æ¯"""
        url_token = self._extract_id(url, r'/people/([\w-]+)')
        if not url_token:
            return {"error": "æ— æ³•æå–ç”¨æˆ·æ ‡è¯†"}

        api_url = f"https://www.zhihu.com/api/v4/members/{url_token}"
        params = {"include": "follower_count,following_count,answer_count,articles_count,voteup_count,thanked_count,description,headline,badge"}

        try:
            response = self.session.get(api_url, params=params, timeout=15)
            response.raise_for_status()
            data = response.json()

            return {
                "type": "user",
                "id": data.get("id"),
                "url_token": url_token,
                "name": data.get("name"),
                "headline": data.get("headline"),
                "description": data.get("description"),
                "avatar": data.get("avatar_url"),
                "gender": data.get("gender"),  # 0æœªçŸ¥ 1ç”· 2å¥³
                "follower_count": data.get("follower_count", 0),
                "following_count": data.get("following_count", 0),
                "answer_count": data.get("answer_count", 0),
                "articles_count": data.get("articles_count", 0),
                "voteup_count": data.get("voteup_count", 0),
                "thanked_count": data.get("thanked_count", 0),
                "badges": [b.get("description") for b in data.get("badge", [])],
                "url": f"https://www.zhihu.com/people/{url_token}"
            }
        except Exception as e:
            return {"error": f"è·å–ç”¨æˆ·ä¿¡æ¯å¤±è´¥: {str(e)}"}

    def fetch_column(self, url: str) -> Dict[str, Any]:
        """æŠ“å–çŸ¥ä¹ä¸“æ ä¿¡æ¯"""
        column_id = self._extract_id(url, r'/column/([\w-]+)')
        if not column_id:
            return {"error": "æ— æ³•æå–ä¸“æ æ ‡è¯†"}

        api_url = f"https://www.zhihu.com/api/v4/columns/{column_id}"

        try:
            response = self.session.get(api_url, timeout=15)
            response.raise_for_status()
            data = response.json()

            author = data.get("author", {})

            # è·å–æœ€æ–°æ–‡ç« 
            articles_url = f"https://www.zhihu.com/api/v4/columns/{column_id}/items"
            articles_params = {"limit": 10}
            articles_resp = self.session.get(articles_url, params=articles_params, timeout=15)
            articles_data = articles_resp.json() if articles_resp.status_code == 200 else {}

            return {
                "type": "column",
                "id": column_id,
                "title": data.get("title"),
                "description": data.get("description"),
                "intro": data.get("intro"),
                "image_url": data.get("image_url"),
                "articles_count": data.get("articles_count", 0),
                "followers_count": data.get("followers", 0),
                "author": {
                    "name": author.get("name"),
                    "headline": author.get("headline"),
                    "url_token": author.get("url_token")
                },
                "recent_articles": self._parse_column_articles(articles_data.get("data", [])),
                "url": f"https://www.zhihu.com/column/{column_id}"
            }
        except Exception as e:
            return {"error": f"è·å–ä¸“æ å¤±è´¥: {str(e)}"}

    def _parse_answers(self, answers: List[Dict]) -> List[Dict]:
        """è§£æå›ç­”åˆ—è¡¨"""
        result = []
        for ans in answers[:5]:
            author = ans.get("author", {})
            result.append({
                "id": ans.get("id"),
                "excerpt": ans.get("excerpt", "")[:200],
                "voteup_count": ans.get("voteup_count", 0),
                "comment_count": ans.get("comment_count", 0),
                "author": {
                    "name": author.get("name"),
                    "headline": author.get("headline")
                }
            })
        return result

    def _parse_column_articles(self, articles: List[Dict]) -> List[Dict]:
        """è§£æä¸“æ æ–‡ç« åˆ—è¡¨"""
        result = []
        for art in articles[:10]:
            result.append({
                "id": art.get("id"),
                "title": art.get("title"),
                "excerpt": art.get("excerpt", "")[:100],
                "voteup_count": art.get("voteup_count", 0),
                "comment_count": art.get("comment_count", 0),
                "created": art.get("created")
            })
        return result

    def _clean_html(self, html: str) -> str:
        """æ¸…ç†HTMLæ ‡ç­¾ï¼Œä¿ç•™çº¯æ–‡æœ¬"""
        if not html:
            return ""
        # ç§»é™¤HTMLæ ‡ç­¾
        text = re.sub(r'<[^>]+>', '', html)
        # æ¸…ç†å¤šä½™ç©ºç™½
        text = re.sub(r'\s+', ' ', text).strip()
        return text

# ==================== ä¸»æ§åˆ¶å™¨ ====================
class WebFetchPlus:
    """WebFetchPlusä¸»æ§åˆ¶å™¨"""

    def __init__(self, config_path: str = None):
        self.config = self._load_config(config_path)
        self.twitter_fetcher = TwitterFetcher(self.config.twitter_api_key) if self.config.twitter_api_key else None
        self.social_fetcher = SocialMediaFetcher()
        self.zhihu_fetcher = ZhihuFetcher()
        self.stats = {"total_fetches": 0, "success": 0, "failed": 0}

    def _load_config(self, config_path: str = None) -> FetchConfig:
        """åŠ è½½é…ç½®"""
        config = FetchConfig()

        # ä»ç¯å¢ƒå˜é‡åŠ è½½
        config.twitter_api_key = os.environ.get("TWITTER_API_KEY", "new1_0a63d55aad214ca990884b2b6637419f")
        config.firecrawl_api_key = os.environ.get("FIRECRAWL_API_KEY", "")

        # ä»é…ç½®æ–‡ä»¶åŠ è½½
        if config_path and os.path.exists(config_path):
            with open(config_path) as f:
                data = json.load(f)
                for key, value in data.items():
                    if hasattr(config, key):
                        setattr(config, key, value)

        return config

    def fetch(self, url: str, strategy: Optional[FetchStrategy] = None, **kwargs) -> Dict[str, Any]:
        """
        æ™ºèƒ½æŠ“å–å†…å®¹

        Args:
            url: è¦æŠ“å–çš„URL
            strategy: æŒ‡å®šç­–ç•¥ï¼ˆå¯é€‰ï¼Œé»˜è®¤è‡ªåŠ¨æ£€æµ‹ï¼‰
            **kwargs: é¢å¤–å‚æ•°

        Returns:
            æŠ“å–ç»“æœå­—å…¸
        """
        self.stats["total_fetches"] += 1

        # è‡ªåŠ¨æ£€æµ‹ç­–ç•¥
        if strategy is None:
            strategy = URLDetector.detect(url)

        print(f"[WebFetchPlus] URL: {url}")
        print(f"[WebFetchPlus] ç­–ç•¥: {strategy.value}")

        try:
            result = self._dispatch_fetch(url, strategy, **kwargs)

            if "error" not in result:
                self.stats["success"] += 1
                result["_metadata"] = {
                    "url": url,
                    "strategy": strategy.value,
                    "timestamp": datetime.now().isoformat(),
                    "success": True
                }
            else:
                self.stats["failed"] += 1
                result["_metadata"] = {
                    "url": url,
                    "strategy": strategy.value,
                    "timestamp": datetime.now().isoformat(),
                    "success": False
                }

            return result

        except Exception as e:
            self.stats["failed"] += 1
            return {
                "error": f"æŠ“å–å¤±è´¥: {str(e)}",
                "_metadata": {
                    "url": url,
                    "strategy": strategy.value,
                    "timestamp": datetime.now().isoformat(),
                    "success": False
                }
            }

    def _dispatch_fetch(self, url: str, strategy: FetchStrategy, **kwargs) -> Dict[str, Any]:
        """åˆ†å‘åˆ°å…·ä½“çš„æŠ“å–å™¨"""

        if strategy == FetchStrategy.TWITTER_API:
            if not self.twitter_fetcher:
                return {"error": "Twitter APIæœªé…ç½®"}

            # åˆ¤æ–­æ˜¯æ¨æ–‡è¿˜æ˜¯ç”¨æˆ·é¡µé¢
            if "/status/" in url:
                return self.twitter_fetcher.fetch_tweet(url)
            else:
                return self.twitter_fetcher.fetch_user(url)

        elif strategy == FetchStrategy.DOUYIN:
            return self.social_fetcher.fetch_douyin(url)

        elif strategy == FetchStrategy.BILIBILI:
            return self.social_fetcher.fetch_bilibili(url)

        elif strategy == FetchStrategy.XIAOHONGSHU:
            return self.social_fetcher.fetch_xiaohongshu(url)

        elif strategy == FetchStrategy.ZHIHU:
            return self.zhihu_fetcher.fetch(url)

        elif strategy == FetchStrategy.FIRECRAWL:
            return self._fetch_with_firecrawl(url, **kwargs)

        else:
            return self._fetch_generic(url, **kwargs)

    def _fetch_with_firecrawl(self, url: str, **kwargs) -> Dict[str, Any]:
        """ä½¿ç”¨FirecrawlæŠ“å–"""
        if not self.config.firecrawl_api_key:
            return self._fetch_generic(url, **kwargs)  # é™çº§åˆ°é€šç”¨æŠ“å–

        # TODO: å®ç°Firecrawl APIè°ƒç”¨
        return {"message": "Firecrawlé›†æˆå¾…å®Œå–„", "url": url}

    def _fetch_generic(self, url: str, **kwargs) -> Dict[str, Any]:
        """é€šç”¨HTTPæŠ“å–"""
        headers = kwargs.get("headers", {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        })

        try:
            response = requests.get(
                url,
                headers=headers,
                timeout=self.config.timeout,
                proxies={"http": self.config.proxy, "https": self.config.proxy} if self.config.proxy else None
            )
            response.raise_for_status()

            # åˆ¤æ–­å†…å®¹ç±»å‹
            content_type = response.headers.get("content-type", "")

            if "json" in content_type:
                return {"content": response.json(), "type": "json"}
            elif "html" in content_type or "text" in content_type:
                return {"content": response.text[:10000], "type": "html"}  # é™åˆ¶é•¿åº¦
            else:
                return {"content": f"äºŒè¿›åˆ¶å†…å®¹ ({len(response.content)} bytes)", "type": "binary"}

        except Exception as e:
            return {"error": f"HTTPè¯·æ±‚å¤±è´¥: {str(e)}"}

    def batch_fetch(self, urls: List[str], **kwargs) -> List[Dict[str, Any]]:
        """æ‰¹é‡æŠ“å–"""
        results = []
        for url in urls:
            result = self.fetch(url, **kwargs)
            results.append(result)
        return results

    def search_twitter(self, query: str, max_results: int = 10) -> Dict[str, Any]:
        """æœç´¢Twitter"""
        if not self.twitter_fetcher:
            return {"error": "Twitter APIæœªé…ç½®"}
        return self.twitter_fetcher.search_tweets(query, max_results)

    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            **self.stats,
            "success_rate": f"{self.stats['success'] / max(self.stats['total_fetches'], 1) * 100:.1f}%"
        }

# ==================== CLIå…¥å£ ====================
def main():
    import argparse

    parser = argparse.ArgumentParser(description="WebFetchPlus - æ™ºèƒ½ç½‘é¡µå†…å®¹æŠ“å–å·¥å…·")
    parser.add_argument("url", nargs="?", help="è¦æŠ“å–çš„URL")
    parser.add_argument("-s", "--search", help="æœç´¢Twitter")
    parser.add_argument("-b", "--batch", help="æ‰¹é‡æŠ“å–ï¼ˆæ–‡ä»¶è·¯å¾„ï¼‰")
    parser.add_argument("--strategy", choices=["auto", "twitter", "firecrawl", "douyin", "bilibili", "zhihu"],
                       default="auto", help="æŒ‡å®šæŠ“å–ç­–ç•¥")
    parser.add_argument("--json", action="store_true", help="è¾“å‡ºJSONæ ¼å¼")
    parser.add_argument("--stats", action="store_true", help="æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯")
    parser.add_argument("--config", help="é…ç½®æ–‡ä»¶è·¯å¾„")

    args = parser.parse_args()

    # åˆå§‹åŒ–
    fetcher = WebFetchPlus(config_path=args.config)

    # å¤„ç†ä¸åŒå‘½ä»¤
    if args.stats:
        print(json.dumps(fetcher.get_stats(), indent=2, ensure_ascii=False))

    elif args.search:
        result = fetcher.search_twitter(args.search)
        if args.json:
            print(json.dumps(result, indent=2, ensure_ascii=False))
        else:
            if "results" in result:
                for tweet in result["results"]:
                    print(f"\n@{tweet['author']['username']}: {tweet['text'][:100]}...")
                    print(f"  â¤ï¸ {tweet['metrics']['like_count']} ğŸ” {tweet['metrics']['retweet_count']}")
            else:
                print(result)

    elif args.batch:
        with open(args.batch) as f:
            urls = [line.strip() for line in f if line.strip()]
        results = fetcher.batch_fetch(urls)
        print(json.dumps(results, indent=2, ensure_ascii=False))

    elif args.url:
        # æ˜ å°„ç­–ç•¥
        strategy_map = {
            "twitter": FetchStrategy.TWITTER_API,
            "firecrawl": FetchStrategy.FIRECRAWL,
            "douyin": FetchStrategy.DOUYIN,
            "bilibili": FetchStrategy.BILIBILI,
            "zhihu": FetchStrategy.ZHIHU
        }
        strategy = strategy_map.get(args.strategy) if args.strategy != "auto" else None

        result = fetcher.fetch(args.url, strategy=strategy)

        if args.json:
            print(json.dumps(result, indent=2, ensure_ascii=False))
        else:
            if "error" in result:
                print(f"âŒ é”™è¯¯: {result['error']}")
            else:
                print(f"âœ… æŠ“å–æˆåŠŸ!")
                print(json.dumps(result, indent=2, ensure_ascii=False)[:1000] + "...")

    else:
        parser.print_help()

if __name__ == "__main__":
    main()