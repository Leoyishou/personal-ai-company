#!/usr/bin/env python3
"""
ç”Ÿæˆå¢å¼ºéŸ³é¢‘çš„ LRC æ­Œè¯æ–‡ä»¶

ç”±äºå¢å¼ºéŸ³é¢‘æœ‰æ…¢é€Ÿæ®µå’Œ TTS æ’å…¥ï¼Œæ—¶é—´è½´éœ€è¦é‡æ–°è®¡ç®—ã€‚
"""

import argparse
import json
import sys
from pathlib import Path
from dataclasses import dataclass
from typing import Optional

sys.path.insert(0, str(Path(__file__).parent))

from asr_processor import extract_definite_sentences, filter_by_duration, load_asr_file
from vocab_matcher import find_unfamiliar_words, get_known_words_set
from audio_mixer import get_audio_duration_ms


@dataclass
class EnhancementPoint:
    """å¢å¼ºç‚¹"""
    sentence_text: str
    start_ms: int
    end_ms: int
    unfamiliar_words: list[str]
    tts_text: str


def select_enhancement_points(
    sentences: list[dict],
    known_words: set[str],
    max_per_minute: int = 3,
    min_interval_ms: int = 10000
) -> list[EnhancementPoint]:
    """é€‰æ‹©éœ€è¦å¢å¼ºçš„å¥å­ï¼ˆä¸ enhance_video.py ä¿æŒä¸€è‡´ï¼‰"""

    DEFINITIONS = {
        "terabyte": "TBï¼Œä¸€ä¸‡äº¿å­—èŠ‚",
        "megapixels": "ç™¾ä¸‡åƒç´ ",
        "gigabytes": "GBï¼Œåäº¿å­—èŠ‚",
        "gigs": "GBçš„å£è¯­è¯´æ³•",
        "milliamp": "æ¯«å®‰ï¼Œç”µæ± å®¹é‡å•ä½",
        "watts": "ç“¦ç‰¹ï¼ŒåŠŸç‡å•ä½",
        "stereo": "ç«‹ä½“å£°",
        "vertu": "Vertuï¼Œè‹±å›½å¥¢ä¾ˆæ‰‹æœºå“ç‰Œ",
        "virtu": "Vertuå“ç‰Œ",
        "specs": "è§„æ ¼å‚æ•°",
        "flagship": "æ——èˆ°äº§å“",
        "amoled": "AMOLEDå±å¹•",
        "bucks": "ç¾å…ƒï¼ˆå£è¯­ï¼‰",
    }

    def generate_tts_text(words: list[str]) -> str:
        parts = []
        for word in words[:3]:
            lower_word = word.lower()
            if lower_word in DEFINITIONS:
                parts.append(f"{word}ï¼Œ{DEFINITIONS[lower_word]}")
            else:
                parts.append(word)
        return "ã€‚".join(parts)

    candidates = []
    for s in sentences:
        unfamiliar = find_unfamiliar_words(s["text"], known_words)
        if unfamiliar:
            tts_text = generate_tts_text(unfamiliar)
            candidates.append(EnhancementPoint(
                sentence_text=s["text"],
                start_ms=s["start_time"],
                end_ms=s["end_time"],
                unfamiliar_words=unfamiliar,
                tts_text=tts_text,
            ))

    candidates.sort(key=lambda x: len(x.unfamiliar_words), reverse=True)

    selected = []
    for candidate in candidates:
        too_close = False
        for existing in selected:
            if abs(candidate.start_ms - existing.start_ms) < min_interval_ms:
                too_close = True
                break

        if not too_close:
            selected.append(candidate)

        duration_minutes = (candidate.end_ms / 1000) / 60 + 1
        if len(selected) >= max_per_minute * duration_minutes:
            break

    selected.sort(key=lambda x: x.start_ms)
    return selected


def estimate_tts_duration_ms(text: str) -> int:
    """ä¼°ç®— TTS éŸ³é¢‘æ—¶é•¿ï¼ˆæ¯«ç§’ï¼‰"""
    # ä¸­æ–‡çº¦ 4 å­—/ç§’ï¼ŒåŠ ä¸Šå‰åé™éŸ³å¡«å……
    char_count = len(text)
    speech_ms = int(char_count / 4 * 1000)
    padding_ms = 800 + 1000  # å‰ 0.8s + å 1s
    return speech_ms + padding_ms


def build_enhanced_timeline(
    sentences: list[dict],
    enhancement_points: list[EnhancementPoint],
    max_duration_ms: int
) -> list[tuple[int, str, Optional[str]]]:
    """
    æ„å»ºå¢å¼ºéŸ³é¢‘çš„æ—¶é—´è½´

    Returns:
        list of (enhanced_time_ms, text, note) - note æ ‡è®°æ˜¯å¦ä¸ºè®²è§£
    """
    timeline = []
    current_enhanced_ms = 0
    current_original_ms = 0

    ep_index = 0

    for s in sentences:
        if s["end_time"] > max_duration_ms:
            break

        # æ£€æŸ¥æ˜¯å¦æœ‰å¢å¼ºç‚¹
        is_enhanced = False
        ep = None
        if ep_index < len(enhancement_points):
            ep = enhancement_points[ep_index]
            if s["start_time"] == ep.start_ms:
                is_enhanced = True

        if is_enhanced:
            # è®¡ç®—ä»ä¸Šä¸€ä¸ªä½ç½®åˆ°å¢å¼ºç‚¹çš„æ—¶é—´
            gap_ms = ep.start_ms - current_original_ms
            current_enhanced_ms += gap_ms

            # 1. æ…¢é€Ÿæ’­æ”¾ï¼ˆ0.7x = æ—¶é•¿ / 0.7ï¼‰
            sentence_duration = ep.end_ms - ep.start_ms
            slow_duration = int(sentence_duration / 0.7)
            timeline.append((current_enhanced_ms, f"ğŸ¢ {s['text']}", "slow"))
            current_enhanced_ms += slow_duration

            # 2. TTS è®²è§£
            tts_duration = estimate_tts_duration_ms(ep.tts_text)
            words_str = ", ".join(ep.unfamiliar_words[:3])
            timeline.append((current_enhanced_ms, f"ğŸ“– {words_str}", "tts"))
            current_enhanced_ms += tts_duration

            # 3. åŸé€Ÿé‡æ’­
            timeline.append((current_enhanced_ms, f"ğŸ” {s['text']}", "replay"))
            current_enhanced_ms += sentence_duration

            current_original_ms = ep.end_ms
            ep_index += 1
        else:
            # æ™®é€šå¥å­ï¼Œæ—¶é—´å·®ç´¯åŠ 
            gap_ms = s["start_time"] - current_original_ms
            current_enhanced_ms += gap_ms

            timeline.append((current_enhanced_ms, s["text"], None))

            sentence_duration = s["end_time"] - s["start_time"]
            current_enhanced_ms += sentence_duration
            current_original_ms = s["end_time"]

    return timeline


def format_lrc_time(ms: int) -> str:
    """æ ¼å¼åŒ–ä¸º LRC æ—¶é—´ [mm:ss.xx]"""
    total_sec = ms / 1000
    minutes = int(total_sec // 60)
    seconds = total_sec % 60
    return f"[{minutes:02d}:{seconds:05.2f}]"


def generate_lrc(
    asr_file: str,
    output_file: str,
    max_duration_ms: Optional[int] = None,
    title: str = "Enhanced Audio",
    artist: str = "Learn English"
) -> str:
    """ç”Ÿæˆ LRC æ­Œè¯æ–‡ä»¶"""

    print(f"Loading ASR data from {asr_file}...")
    asr_data = load_asr_file(asr_file)

    print("Extracting definite sentences...")
    sentences = extract_definite_sentences(asr_data)

    if max_duration_ms:
        sentences = filter_by_duration(sentences, max_duration_ms)

    print("Loading vocabulary...")
    known_words = get_known_words_set()

    print("Selecting enhancement points...")
    enhancement_points = select_enhancement_points(sentences, known_words)

    print(f"Building timeline for {len(sentences)} sentences, {len(enhancement_points)} enhancement points...")
    timeline = build_enhanced_timeline(sentences, enhancement_points, max_duration_ms or 999999999)

    # ç”Ÿæˆ LRC
    lines = [
        f"[ti:{title}]",
        f"[ar:{artist}]",
        "[by:learn-english skill]",
        "",
    ]

    for time_ms, text, note in timeline:
        lrc_time = format_lrc_time(time_ms)
        lines.append(f"{lrc_time}{text}")

    lrc_content = "\n".join(lines)

    with open(output_file, "w", encoding="utf-8") as f:
        f.write(lrc_content)

    print(f"LRC saved to {output_file}")
    print(f"Total lines: {len(timeline)}")

    return output_file


def embed_lrc_to_mp3(mp3_file: str, lrc_file: str, output_file: str) -> str:
    """å°† LRC æ­Œè¯åµŒå…¥ MP3ï¼ˆä½¿ç”¨ eyeD3ï¼‰"""
    import subprocess

    # è¯»å– LRC å†…å®¹
    with open(lrc_file, "r", encoding="utf-8") as f:
        lrc_content = f.read()

    # å…ˆå¤åˆ¶æ–‡ä»¶
    if mp3_file != output_file:
        subprocess.run(["cp", mp3_file, output_file], check=True)

    # ä½¿ç”¨ eyeD3 åµŒå…¥æ­Œè¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    try:
        import eyed3
        audiofile = eyed3.load(output_file)
        if audiofile.tag is None:
            audiofile.initTag()

        # æ·»åŠ åŒæ­¥æ­Œè¯
        audiofile.tag.lyrics.set(lrc_content, description="Enhanced")
        audiofile.tag.save()
        print(f"Lyrics embedded to {output_file}")
    except ImportError:
        # å›é€€ï¼šä½¿ç”¨ ffmpeg æ·»åŠ å…ƒæ•°æ®
        print("eyeD3 not available, using ffmpeg metadata...")
        temp_file = output_file + ".tmp.mp3"
        subprocess.run([
            "/opt/homebrew/Cellar/ffmpeg/8.0.1_2/bin/ffmpeg",
            "-y", "-hide_banner", "-loglevel", "warning",
            "-i", output_file,
            "-metadata", f"lyrics={lrc_content[:500]}",  # ffmpeg lyrics æœ‰é•¿åº¦é™åˆ¶
            "-c", "copy",
            temp_file
        ], check=True)
        subprocess.run(["mv", temp_file, output_file], check=True)
        print(f"Metadata added to {output_file} (truncated due to ffmpeg limit)")
        print(f"Tip: Copy {lrc_file} alongside the MP3 for full lyrics support")

    return output_file


def main():
    parser = argparse.ArgumentParser(description="ç”Ÿæˆå¢å¼ºéŸ³é¢‘çš„ LRC æ­Œè¯")
    parser.add_argument("--asr", "-a", required=True, help="ASR JSON æ–‡ä»¶")
    parser.add_argument("--output", "-o", required=True, help="è¾“å‡º LRC æ–‡ä»¶")
    parser.add_argument("--mp3", "-m", help="è¦åµŒå…¥æ­Œè¯çš„ MP3 æ–‡ä»¶")
    parser.add_argument("--duration", "-d", type=int, help="åªå¤„ç†å‰ N ç§’")
    parser.add_argument("--title", "-t", default="Enhanced Audio", help="æ­Œæ›²æ ‡é¢˜")

    args = parser.parse_args()

    max_duration_ms = args.duration * 1000 if args.duration else None

    generate_lrc(
        asr_file=args.asr,
        output_file=args.output,
        max_duration_ms=max_duration_ms,
        title=args.title,
    )

    if args.mp3:
        output_mp3 = args.mp3.replace(".mp3", "_with_lyrics.mp3")
        embed_lrc_to_mp3(args.mp3, args.output, output_mp3)


if __name__ == "__main__":
    main()
