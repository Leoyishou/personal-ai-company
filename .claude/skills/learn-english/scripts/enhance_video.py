#!/usr/bin/env python3
"""
è§†é¢‘è‹±è¯­å­¦ä¹ å¢å¼ºå·¥å…· - ä¸»å…¥å£

åŠŸèƒ½ï¼š
1. ä» ASR ç»“æœä¸­æå–ç²¾ç¡®å¥å­
2. è¯†åˆ«åŒ…å«ç”Ÿè¯çš„å¥å­
3. å¯¹è¿™äº›å¥å­è¿›è¡Œæ…¢é€Ÿæ’­æ”¾ + TTS è®²è§£ + åŸé€Ÿé‡æ’­
4. ç”Ÿæˆå¢å¼ºåçš„éŸ³é¢‘
5. ç”Ÿæˆå­¦ä¹ ç¬”è®° notes.txt

ä½¿ç”¨æ–¹æ³•ï¼š
    python enhance_video.py \
        --audio /path/to/source.mp3 \
        --asr /path/to/asr.json \
        --output-dir ~/Library/Mobile Documents/com~apple~CloudDocs/practiceEngListen/260202_example \
        --duration 900
"""

import argparse
import json
import os
import subprocess
import sys
import tempfile
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Optional

# æ·»åŠ å½“å‰ç›®å½•åˆ° path
sys.path.insert(0, str(Path(__file__).parent))

from asr_processor import extract_definite_sentences, filter_by_duration, load_asr_file
from vocab_matcher import find_unfamiliar_words, get_known_words_set
from audio_mixer import AudioMixer, cut_audio, slow_down_audio, normalize_audio, concat_audios, get_audio_duration_ms, FFMPEG_PATH


# TTS è„šæœ¬è·¯å¾„
TTS_SCRIPT = Path.home() / ".claude" / "skills" / "api-tts" / "scripts" / "tts.py"

# OpenRouter è„šæœ¬è·¯å¾„
OPENROUTER_SCRIPT = Path.home() / ".claude" / "skills" / "api-openrouter" / "scripts" / "openrouter_chat.py"

# iCloud practiceEngListen ç›®å½•
ICLOUD_BASE = Path.home() / "Library" / "Mobile Documents" / "com~apple~CloudDocs" / "practiceEngListen"

# é¢„ç½®è¯æ±‡é‡Šä¹‰ï¼ˆé€šä¿—ç‰ˆï¼‰
DEFINITIONS = {
    # å­˜å‚¨ç›¸å…³
    "terabyte": "TBï¼Œå¤§çº¦èƒ½å­˜ä¸€åƒéƒ¨é«˜æ¸…ç”µå½±",
    "megapixels": "ç™¾ä¸‡åƒç´ ï¼Œæ•°å­—è¶Šå¤§ç…§ç‰‡è¶Šæ¸…æ™°",
    "gigabytes": "GBï¼Œæ‰‹æœºå­˜å‚¨çš„å¸¸ç”¨å•ä½",
    "gigs": "GBçš„å£è¯­è¯´æ³•",
    # ç”µæ± ç›¸å…³
    "milliamp": "æ¯«å®‰ï¼Œç”µæ± å®¹é‡å•ä½",
    "watts": "ç“¦ç‰¹ï¼Œå……ç”µåŠŸç‡å•ä½",
    # éŸ³é¢‘ç›¸å…³
    "stereo": "ç«‹ä½“å£°ï¼Œå·¦å³è€³å¬åˆ°ä¸åŒå£°éŸ³",
    # å“ç‰Œç›¸å…³
    "vertu": "Vertuï¼Œè‹±å›½å¥¢ä¾ˆæ‰‹æœºå“ç‰Œ",
    "virtu": "Vertuçš„å¦ä¸€ç§æ‹¼æ³•",
    # æŠ€æœ¯æœ¯è¯­
    "specs": "è§„æ ¼å‚æ•°çš„ç¼©å†™",
    "flagship": "æ——èˆ°ï¼Œæœ€é«˜ç«¯çš„äº§å“",
    # å¸¸è§ä¿šè¯­
    "gooning": "æ²‰è¿·äºæŸäº‹æ— æ³•è‡ªæ‹”",
    "edging": "æ•…æ„æ‹–å»¶è¾¾åˆ°é«˜æ½®",
    "binges": "æš´é¥®æš´é£Ÿæˆ–æ²‰è¿·è¡Œä¸º",
}

# éš¾åº¦åˆ¤æ–­é˜ˆå€¼
DIFFICULTY_THRESHOLDS = {
    "min_unfamiliar_words": 2,  # ç”Ÿè¯ >= 2 ä¸ªè§†ä¸ºå›°éš¾ï¼Œéœ€è¦æ•´å¥è®²è§£
}


@dataclass
class EnhancementPoint:
    """å¢å¼ºç‚¹"""
    sentence_text: str
    start_ms: int
    end_ms: int
    unfamiliar_words: list[str]
    tts_text: str
    sentence_translation: str = ""  # æ•´å¥ç¿»è¯‘
    word_explanations: list[dict] = field(default_factory=list)  # ç”Ÿè¯è¯¦è§£
    is_difficult: bool = False  # æ˜¯å¦éœ€è¦æ•´å¥è®²è§£


def is_difficult_sentence(sentence: str, unfamiliar_words: list[str]) -> bool:
    """
    åˆ¤æ–­å¥å­æ˜¯å¦éš¾ä»¥ç†è§£ï¼Œéœ€è¦æ•´å¥è®²è§£

    æ¡ä»¶ï¼šç”Ÿè¯ >= 2 ä¸ª

    Args:
        sentence: åŸå¥
        unfamiliar_words: ç”Ÿè¯åˆ—è¡¨

    Returns:
        æ˜¯å¦éš¾ä»¥ç†è§£
    """
    return len(unfamiliar_words) >= DIFFICULTY_THRESHOLDS["min_unfamiliar_words"]


def call_openrouter_for_explanation(sentence: str, unfamiliar_words: list[str]) -> tuple[str, list[dict]]:
    """
    è°ƒç”¨ OpenRouter AI ç¿»è¯‘å¥å­å¹¶è§£é‡Šç”Ÿè¯

    Args:
        sentence: åŸå¥
        unfamiliar_words: ç”Ÿè¯åˆ—è¡¨

    Returns:
        (æ•´å¥ç¿»è¯‘, ç”Ÿè¯è§£é‡Šåˆ—è¡¨)
    """
    words_str = ", ".join(unfamiliar_words[:5])

    prompt = f"""ä½ æ˜¯è‹±è¯­å­¦ä¹ åŠ©æ‰‹ã€‚ç”¨æˆ·æ˜¯ä¸­å›½äººï¼Œè‹±è¯­æ°´å¹³ C1ï¼ˆæ‰˜ç¦ 100+ï¼Œè¯æ±‡é‡çº¦ 14000ï¼‰ã€‚

è¯·ç¿»è¯‘è¿™å¥è¯ï¼Œå¹¶è§£é‡Šå…¶ä¸­çš„ç”Ÿè¯ã€‚è¦æ±‚ï¼š
1. ç¿»è¯‘è¦é€šä¿—è‡ªç„¶ï¼Œç”¨å¤§ç™½è¯
2. ç”Ÿè¯è§£é‡Šè¦ç®€æ´ï¼Œ10 å­—ä»¥å†…
3. åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–å†…å®¹

åŸå¥ï¼š{sentence}

ç”Ÿè¯ï¼š{words_str}

è¿”å›æ ¼å¼ï¼š
{{"translation": "æ•´å¥ç¿»è¯‘", "words": [{{"word": "xxx", "meaning": "ç®€çŸ­è§£é‡Š"}}]}}"""

    try:
        result = subprocess.run(
            ["python", str(OPENROUTER_SCRIPT), "--prompt", prompt, "--model", "google/gemini-2.0-flash-001"],
            capture_output=True,
            text=True,
            timeout=30,
            cwd=str(OPENROUTER_SCRIPT.parent.parent),
        )

        if result.returncode == 0:
            output = result.stdout.strip()
            # æå– JSON éƒ¨åˆ†
            if "{" in output and "}" in output:
                json_start = output.find("{")
                json_end = output.rfind("}") + 1
                json_str = output[json_start:json_end]
                data = json.loads(json_str)
                translation = data.get("translation", "")
                words = data.get("words", [])
                return translation, words
    except Exception as e:
        print(f"  OpenRouter call failed: {e}")

    return "", []


def load_definitions(file_path: Optional[str] = None) -> dict:
    """åŠ è½½è¯æ±‡é‡Šä¹‰"""
    definitions = DEFINITIONS.copy()

    if file_path and Path(file_path).exists():
        with open(file_path) as f:
            custom = json.load(f)
            definitions.update(custom)

    return definitions


def generate_tts_text(words: list[str], sentence: str, definitions: dict) -> tuple[str, str, list[dict]]:
    """
    ç”Ÿæˆ TTS è®²è§£æ–‡æœ¬ï¼ˆé€šä¿—æ˜“æ‡‚ç‰ˆï¼‰

    Args:
        words: ç”Ÿè¯åˆ—è¡¨
        sentence: åŸå¥
        definitions: è¯æ±‡é‡Šä¹‰å­—å…¸

    Returns:
        (tts_text, sentence_translation, word_explanations)
    """
    parts = []
    word_explanations = []

    for word in words[:3]:  # æœ€å¤šè®²è§£ 3 ä¸ªè¯
        lower_word = word.lower()
        if lower_word in definitions:
            explanation = definitions[lower_word]
            parts.append(f"{word}ï¼Œ{explanation}")
            word_explanations.append({"word": word, "meaning": explanation})
        else:
            # æ²¡æœ‰é¢„ç½®é‡Šä¹‰ï¼Œåªè¯»å•è¯
            parts.append(word)
            word_explanations.append({"word": word, "meaning": "ï¼ˆéœ€æŸ¥è¯å…¸ï¼‰"})

    tts_text = "ã€‚".join(parts)

    # æ•´å¥ç¿»è¯‘ç•™ç©ºï¼Œç”±è°ƒç”¨è€…ç”¨ AI è¡¥å……
    sentence_translation = ""

    return tts_text, sentence_translation, word_explanations


def call_tts(text: str, output_path: str) -> str:
    """
    è°ƒç”¨ TTS è„šæœ¬ç”Ÿæˆè¯­éŸ³

    Args:
        text: å¾…åˆæˆæ–‡æœ¬
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„

    Returns:
        è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    cmd = [
        "python", str(TTS_SCRIPT),
        text,
        "--voice", "yangyang",  # ä½¿ç”¨ç‚€ç‚€éŸ³è‰²
        "--output", output_path,
    ]

    result = subprocess.run(cmd, capture_output=True, text=True)
    if result.returncode != 0:
        print(f"TTS failed: {result.stderr}")
        raise RuntimeError(f"TTS synthesis failed: {result.stderr}")

    return output_path


def select_enhancement_points(
    sentences: list[dict],
    known_words: set[str],
    max_per_minute: int = 3,
    min_interval_ms: int = 10000
) -> list[EnhancementPoint]:
    """
    é€‰æ‹©éœ€è¦å¢å¼ºçš„å¥å­

    è§„åˆ™ï¼š
    - æ¯åˆ†é’Ÿæœ€å¤š max_per_minute ä¸ªå¢å¼ºç‚¹
    - ç›¸é‚»å¢å¼ºé—´éš” >= min_interval_ms
    - ä¼˜å…ˆé€‰æ‹©ç”Ÿè¯å¤šçš„å¥å­

    Args:
        sentences: å¥å­åˆ—è¡¨
        known_words: å·²çŸ¥è¯æ±‡é›†åˆ
        max_per_minute: æ¯åˆ†é’Ÿæœ€å¤§å¢å¼ºæ•°
        min_interval_ms: æœ€å°é—´éš”ï¼ˆæ¯«ç§’ï¼‰

    Returns:
        å¢å¼ºç‚¹åˆ—è¡¨
    """
    definitions = load_definitions()

    # ä¸ºæ¯ä¸ªå¥å­è®¡ç®—ç”Ÿè¯
    candidates = []
    for s in sentences:
        unfamiliar = find_unfamiliar_words(s["text"], known_words)
        if unfamiliar:
            tts_text, sentence_translation, word_explanations = generate_tts_text(
                unfamiliar, s["text"], definitions
            )
            is_diff = is_difficult_sentence(s["text"], unfamiliar)
            candidates.append(EnhancementPoint(
                sentence_text=s["text"],
                start_ms=s["start_time"],
                end_ms=s["end_time"],
                unfamiliar_words=unfamiliar,
                tts_text=tts_text,
                sentence_translation=sentence_translation,
                word_explanations=word_explanations,
                is_difficult=is_diff,
            ))

    # æŒ‰ç”Ÿè¯æ•°é‡é™åºæ’åº
    candidates.sort(key=lambda x: len(x.unfamiliar_words), reverse=True)

    # é€‰æ‹©å¢å¼ºç‚¹ï¼ˆè€ƒè™‘é—´éš”é™åˆ¶ï¼‰
    selected = []
    for candidate in candidates:
        # æ£€æŸ¥ä¸å·²é€‰æ‹©ç‚¹çš„é—´éš”
        too_close = False
        for existing in selected:
            if abs(candidate.start_ms - existing.start_ms) < min_interval_ms:
                too_close = True
                break

        if not too_close:
            selected.append(candidate)

        # æ£€æŸ¥æ¯åˆ†é’Ÿé™åˆ¶
        duration_minutes = (candidate.end_ms / 1000) / 60 + 1
        if len(selected) >= max_per_minute * duration_minutes:
            break

    # æŒ‰æ—¶é—´æ’åº
    selected.sort(key=lambda x: x.start_ms)

    # å¯¹éš¾å¥è°ƒç”¨ AI ç¿»è¯‘
    difficult_count = sum(1 for ep in selected if ep.is_difficult)
    if difficult_count > 0:
        print(f"  Translating {difficult_count} difficult sentences with AI...")
        for ep in selected:
            if ep.is_difficult:
                translation, ai_words = call_openrouter_for_explanation(ep.sentence_text, ep.unfamiliar_words)
                if translation:
                    ep.sentence_translation = translation
                    # ç”¨ AI è§£é‡Šæ›´æ–° TTS æ–‡æœ¬
                    ep.tts_text = f"è¿™å¥è¯çš„æ„æ€æ˜¯ï¼Œ{translation}"
                if ai_words:
                    # åˆå¹¶ AI è§£é‡Šåˆ° word_explanations
                    ai_dict = {w["word"].lower(): w["meaning"] for w in ai_words}
                    for we in ep.word_explanations:
                        if we["word"].lower() in ai_dict:
                            we["meaning"] = ai_dict[we["word"].lower()]

    return selected


def ms_to_time_str(ms: int) -> str:
    """æ¯«ç§’è½¬æ—¶é—´å­—ç¬¦ä¸² MM:SS"""
    total_seconds = ms // 1000
    minutes = total_seconds // 60
    seconds = total_seconds % 60
    return f"{minutes:02d}:{seconds:02d}"


def ms_to_lrc_time(ms: int) -> str:
    """æ¯«ç§’è½¬ LRC æ—¶é—´æ ¼å¼ [mm:ss.xx]"""
    total_sec = ms / 1000
    minutes = int(total_sec // 60)
    seconds = total_sec % 60
    return f"[{minutes:02d}:{seconds:05.2f}]"


@dataclass
class TimelineEntry:
    """æ—¶é—´è½´æ¡ç›®"""
    start_ms: int
    text: str
    entry_type: str  # "normal", "slow", "tts", "replay"
    unfamiliar_words: list[str] = field(default_factory=list)


def mark_unfamiliar_words(text: str, unfamiliar_words: list[str]) -> str:
    """åœ¨æ–‡æœ¬ä¸­æ ‡è®°ç”Ÿè¯ï¼Œç”¨ __word__ æ ¼å¼"""
    if not unfamiliar_words:
        return text

    # åˆ›å»ºå°å†™ç”Ÿè¯é›†åˆç”¨äºåŒ¹é…
    unfamiliar_set = {w.lower() for w in unfamiliar_words}

    # æŒ‰å•è¯åˆ†å‰²ï¼Œä¿ç•™æ ‡ç‚¹
    import re
    # åˆ†å‰²ä¿ç•™å•è¯å’Œéå•è¯éƒ¨åˆ†
    parts = re.split(r'(\b\w+\b)', text)

    result = []
    for part in parts:
        if part and part.lower() in unfamiliar_set:
            result.append(f"__{part}__")
        else:
            result.append(part)

    return ''.join(result)


def generate_lrc_file(
    output_dir: Path,
    timeline: list[TimelineEntry],
    title: str = "Enhanced Audio",
) -> str:
    """
    ç”Ÿæˆ LRC æ­Œè¯æ–‡ä»¶

    Args:
        output_dir: è¾“å‡ºç›®å½•
        timeline: æ—¶é—´è½´æ¡ç›®åˆ—è¡¨
        title: æ ‡é¢˜

    Returns:
        LRC æ–‡ä»¶è·¯å¾„
    """
    lrc_path = output_dir / "enhanced.lrc"

    lines = [
        f"[ti:{title}]",
        "[ar:Learn English]",
        "[by:viva-enhance]",
        "",
    ]

    for entry in timeline:
        lrc_time = ms_to_lrc_time(entry.start_ms)
        # æ ‡è®°ç”Ÿè¯
        marked_text = mark_unfamiliar_words(entry.text, entry.unfamiliar_words)
        # æ ¹æ®ç±»å‹æ·»åŠ æ ‡è®°
        if entry.entry_type == "slow":
            lines.append(f"{lrc_time}ğŸ¢ {marked_text}")
        elif entry.entry_type == "tts":
            lines.append(f"{lrc_time}ğŸ“– {entry.text}")  # TTS ä¸æ ‡è®°ç”Ÿè¯
        elif entry.entry_type == "replay":
            lines.append(f"{lrc_time}ğŸ” {marked_text}")
        else:
            lines.append(f"{lrc_time}{entry.text}")  # æ™®é€šå¥å­ä¸æ ‡è®°

    lrc_content = "\n".join(lines)

    with open(lrc_path, "w", encoding="utf-8") as f:
        f.write(lrc_content)

    return str(lrc_path)


def generate_notes_file(
    output_dir: Path,
    enhancement_points: list[EnhancementPoint],
    source_url: str = "",
    duration_seconds: int = 0,
) -> str:
    """
    ç”Ÿæˆå­¦ä¹ ç¬”è®° notes.txt

    Args:
        output_dir: è¾“å‡ºç›®å½•
        enhancement_points: å¢å¼ºç‚¹åˆ—è¡¨
        source_url: åŸå§‹è§†é¢‘ URL
        duration_seconds: å¤„ç†æ—¶é•¿

    Returns:
        notes.txt è·¯å¾„
    """
    folder_name = output_dir.name
    notes_path = output_dir / "notes.txt"

    lines = [
        f"# {folder_name} è‹±è¯­å­¦ä¹ ç¬”è®°",
        "",
        f"ç”Ÿæˆæ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M')}",
    ]

    if source_url:
        lines.append(f"åŸå§‹è§†é¢‘ï¼š{source_url}")

    if duration_seconds:
        minutes = duration_seconds // 60
        lines.append(f"å¤„ç†æ—¶é•¿ï¼š{minutes} åˆ†é’Ÿ")

    lines.extend(["", "---", ""])

    for i, ep in enumerate(enhancement_points, 1):
        time_range = f"{ms_to_time_str(ep.start_ms)} - {ms_to_time_str(ep.end_ms)}"

        lines.extend([
            f"## å¢å¼ºç‚¹ {i} [{time_range}]",
            "",
            f"åŸæ–‡ï¼š{ep.sentence_text}",
        ])

        if ep.sentence_translation:
            lines.append(f"æ•´å¥æ„æ€ï¼š{ep.sentence_translation}")
        else:
            lines.append("æ•´å¥æ„æ€ï¼šï¼ˆéœ€è¡¥å……ï¼‰")

        lines.extend(["", "ç”Ÿè¯è®²è§£ï¼š"])

        for we in ep.word_explanations:
            lines.append(f"- {we['word']}: {we['meaning']}")

        lines.extend(["", "---", ""])

    notes_content = "\n".join(lines)
    notes_path.write_text(notes_content, encoding="utf-8")

    return str(notes_path)


def calculate_wpm(asr_data: list, max_duration_ms: Optional[int] = None) -> tuple[int, int, int]:
    """
    è®¡ç®— ASR æ•°æ®çš„è¯­é€Ÿ (Words Per Minute)

    Args:
        asr_data: ASR æ®µè½åˆ—è¡¨
        max_duration_ms: æœ€å¤§æ—¶é•¿é™åˆ¶ï¼ˆæ¯«ç§’ï¼‰

    Returns:
        (wpm, total_words, duration_ms)
    """
    definite_segs = [s for s in asr_data if s.get('definite', False)]
    if not definite_segs:
        return 0, 0, 0

    total_words = 0
    max_time = 0
    min_time = float('inf')

    for seg in definite_segs:
        if max_duration_ms and seg['start_time'] > max_duration_ms:
            continue
        words = len(seg['text'].split())
        total_words += words
        if seg['start_time'] < min_time:
            min_time = seg['start_time']
        if seg['end_time'] > max_time:
            max_time = min(seg['end_time'], max_duration_ms) if max_duration_ms else seg['end_time']

    duration_ms = max_time - min_time if max_time > min_time else 0
    duration_minutes = duration_ms / 1000 / 60
    wpm = int(total_words / duration_minutes) if duration_minutes > 0 else 0

    return wpm, total_words, duration_ms


def adjust_speed_if_needed(
    source_audio: str,
    asr_data: list,
    target_wpm: int,
    current_wpm: int,
    work_dir: Path
) -> tuple[str, list, float]:
    """
    å¦‚æœå½“å‰è¯­é€Ÿè¶…è¿‡ç›®æ ‡ï¼Œå‡é€ŸéŸ³é¢‘å¹¶è°ƒæ•´ ASR æ—¶é—´æˆ³

    Returns:
        (adjusted_audio_path, adjusted_asr_data, scale_factor)
    """
    if current_wpm <= target_wpm:
        return source_audio, asr_data, 1.0

    # è®¡ç®—å‡é€Ÿæ¯”ä¾‹
    atempo = target_wpm / current_wpm
    scale_factor = current_wpm / target_wpm

    print(f"  Speed adjustment: {current_wpm} WPM â†’ {target_wpm} WPM (atempo={atempo:.3f})")

    # å‡é€ŸéŸ³é¢‘
    adjusted_audio = work_dir / "speed_adjusted.mp3"
    cmd = [
        FFMPEG_PATH, "-y", "-i", source_audio,
        "-filter:a", f"atempo={atempo}",
        str(adjusted_audio)
    ]
    subprocess.run(cmd, check=True, capture_output=True)

    # è°ƒæ•´ ASR æ—¶é—´æˆ³
    adjusted_asr = []
    for seg in asr_data:
        new_seg = seg.copy()
        new_seg['start_time'] = int(seg['start_time'] * scale_factor)
        new_seg['end_time'] = int(seg['end_time'] * scale_factor)
        adjusted_asr.append(new_seg)

    return str(adjusted_audio), adjusted_asr, scale_factor


def enhance_audio(
    source_audio: str,
    asr_file: str,
    output_dir: str,
    max_duration_ms: Optional[int] = None,
    target_wpm: int = 100,
    source_url: str = "",
    work_dir: Optional[str] = None
) -> tuple[str, str]:
    """
    å¢å¼ºéŸ³é¢‘

    Args:
        source_audio: åŸå§‹éŸ³é¢‘æ–‡ä»¶
        asr_file: ASR JSON æ–‡ä»¶
        output_dir: è¾“å‡ºç›®å½•
        max_duration_ms: æœ€å¤§å¤„ç†æ—¶é•¿ï¼ˆæ¯«ç§’ï¼‰
        target_wpm: ç›®æ ‡è¯­é€Ÿ WPMï¼ˆé»˜è®¤ 100ï¼‰
        source_url: åŸå§‹è§†é¢‘ URL
        work_dir: å·¥ä½œç›®å½•

    Returns:
        (enhanced.mp3 è·¯å¾„, notes.txt è·¯å¾„, enhanced.lrc è·¯å¾„)
    """
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_path = Path(output_dir).expanduser()
    output_path.mkdir(parents=True, exist_ok=True)
    output_file = output_path / "enhanced.mp3"

    print(f"Loading ASR data from {asr_file}...")
    asr_data = load_asr_file(asr_file)

    # åˆ›å»ºä¸´æ—¶å·¥ä½œç›®å½•ï¼ˆè¯­é€Ÿè°ƒæ•´éœ€è¦ï¼‰
    if work_dir:
        work_path = Path(work_dir)
        work_path.mkdir(parents=True, exist_ok=True)
    else:
        work_path = Path(tempfile.mkdtemp())

    # è®¡ç®—å½“å‰è¯­é€Ÿå¹¶è°ƒæ•´
    current_wpm, total_words, _ = calculate_wpm(asr_data, max_duration_ms)
    print(f"  Current speed: {current_wpm} WPM ({total_words} words)")

    if target_wpm and current_wpm > target_wpm:
        source_audio, asr_data, scale_factor = adjust_speed_if_needed(
            source_audio, asr_data, target_wpm, current_wpm, work_path
        )
        # è°ƒæ•´ max_duration_ms
        if max_duration_ms:
            max_duration_ms = int(max_duration_ms * scale_factor)
            print(f"  Adjusted duration limit: {max_duration_ms/1000:.1f}s")

    print("Extracting definite sentences...")
    sentences = extract_definite_sentences(asr_data)
    print(f"  Found {len(sentences)} definite sentences")

    if max_duration_ms:
        sentences = filter_by_duration(sentences, max_duration_ms)
        print(f"  After filtering: {len(sentences)} sentences within {max_duration_ms/1000:.1f}s")

    print("Loading vocabulary from Supabase...")
    known_words = get_known_words_set()
    print(f"  Known words: {len(known_words)}")

    print("Selecting enhancement points...")
    enhancement_points = select_enhancement_points(sentences, known_words)
    print(f"  Selected {len(enhancement_points)} enhancement points:")

    for i, ep in enumerate(enhancement_points, 1):
        difficulty_mark = " [éš¾]" if ep.is_difficult else ""
        print(f"    {i}. [{ep.start_ms/1000:.1f}s - {ep.end_ms/1000:.1f}s]{difficulty_mark} {ep.unfamiliar_words}")

    if not enhancement_points:
        print("No enhancement points found, copying original audio...")
        subprocess.run(["cp", source_audio, str(output_file)], check=True)
        # ç”Ÿæˆç©ºçš„ notes.txt å’Œ LRC
        notes_path = generate_notes_file(
            output_path, [], source_url,
            int(max_duration_ms / 1000) if max_duration_ms else 0
        )
        lrc_path = generate_lrc_file(output_path, [], title=output_path.name)
        return str(output_file), notes_path, lrc_path

    print(f"Work directory: {work_path}")

    # è·å–éŸ³é¢‘æ€»æ—¶é•¿
    total_duration_ms = get_audio_duration_ms(source_audio)
    if max_duration_ms:
        total_duration_ms = min(total_duration_ms, max_duration_ms)

    # æ„å»ºéŸ³é¢‘ç‰‡æ®µåˆ—è¡¨å’Œæ—¶é—´è½´
    parts = []
    timeline: list[TimelineEntry] = []
    current_pos = 0
    part_idx = 0
    enhanced_time_ms = 0  # å¢å¼ºåçš„ç´¯è®¡æ—¶é—´

    # åˆ›å»ºå¢å¼ºç‚¹æ—¶é—´é›†åˆï¼Œç”¨äºåˆ¤æ–­å¥å­æ˜¯å¦æ˜¯å¢å¼ºç‚¹
    enhancement_start_times = {ep.start_ms for ep in enhancement_points}

    def add_normal_sentences_to_timeline(start_ms: int, end_ms: int, base_enhanced_ms: int):
        """å°†æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„æ™®é€šå¥å­æ·»åŠ åˆ° timeline"""
        for s in sentences:
            # å¥å­åœ¨èŒƒå›´å†…ä¸”ä¸æ˜¯å¢å¼ºç‚¹
            if s["start_time"] >= start_ms and s["end_time"] <= end_ms:
                if s["start_time"] not in enhancement_start_times:
                    # è®¡ç®—åœ¨å¢å¼ºåæ—¶é—´è½´ä¸Šçš„ä½ç½®
                    offset = s["start_time"] - start_ms
                    timeline.append(TimelineEntry(
                        start_ms=base_enhanced_ms + offset,
                        text=s["text"],
                        entry_type="normal"
                    ))

    for ep in enhancement_points:
        # 1. å¢å¼ºç‚¹ä¹‹å‰çš„åŸå§‹éŸ³é¢‘
        if ep.start_ms > current_pos:
            part_idx += 1
            before_file = work_path / f"part_{part_idx:03d}_before.mp3"
            cut_audio(source_audio, str(before_file), current_pos, ep.start_ms)
            parts.append(str(before_file))
            # è®°å½•æ™®é€šå¥å­çš„æ—¶é—´è½´
            add_normal_sentences_to_timeline(current_pos, ep.start_ms, enhanced_time_ms)
            before_duration = get_audio_duration_ms(str(before_file))
            enhanced_time_ms += before_duration
            print(f"  Part {part_idx}: before segment [{current_pos/1000:.1f}s - {ep.start_ms/1000:.1f}s]")

        # 2. æ…¢é€Ÿæ’­æ”¾å¢å¼ºå¥å­
        part_idx += 1
        highlight_orig = work_path / f"part_{part_idx:03d}_highlight_orig.mp3"
        cut_audio(source_audio, str(highlight_orig), ep.start_ms, ep.end_ms)

        highlight_slow = work_path / f"part_{part_idx:03d}_highlight_slow.mp3"
        slow_down_audio(str(highlight_orig), str(highlight_slow), tempo=0.7)
        parts.append(str(highlight_slow))
        # è®°å½•æ…¢é€Ÿæ—¶é—´è½´ï¼ˆåŒ…å«ç”Ÿè¯æ ‡è®°ï¼‰
        timeline.append(TimelineEntry(
            start_ms=enhanced_time_ms,
            text=ep.sentence_text,
            entry_type="slow",
            unfamiliar_words=ep.unfamiliar_words
        ))
        slow_duration = get_audio_duration_ms(str(highlight_slow))
        enhanced_time_ms += slow_duration
        print(f"  Part {part_idx}: slow segment [{ep.start_ms/1000:.1f}s - {ep.end_ms/1000:.1f}s] (0.7x)")

        # 3. TTS è®²è§£
        if ep.tts_text:
            part_idx += 1
            tts_raw = work_path / f"part_{part_idx:03d}_tts_raw.mp3"
            tts_padded = work_path / f"part_{part_idx:03d}_tts_padded.mp3"

            print(f"  Generating TTS: {ep.tts_text[:50]}...")
            call_tts(ep.tts_text, str(tts_raw))

            # æ ‡å‡†åŒ–å¹¶æ·»åŠ é™éŸ³å¡«å……
            tts_norm = work_path / f"part_{part_idx:03d}_tts_norm.mp3"
            normalize_audio(str(tts_raw), str(tts_norm))

            # ä½¿ç”¨ ffmpeg æ·»åŠ é™éŸ³
            subprocess.run([
                FFMPEG_PATH, "-y", "-hide_banner", "-loglevel", "warning",
                "-i", str(tts_norm),
                "-af", "adelay=800|800,apad=pad_dur=1",
                "-ar", "44100", "-ac", "2", "-b:a", "128k",
                str(tts_padded)
            ], check=True)

            parts.append(str(tts_padded))
            # è®°å½• TTS æ—¶é—´è½´
            timeline.append(TimelineEntry(
                start_ms=enhanced_time_ms,
                text=ep.tts_text,
                entry_type="tts"
            ))
            tts_duration = get_audio_duration_ms(str(tts_padded))
            enhanced_time_ms += tts_duration
            print(f"  Part {part_idx}: TTS with padding")

        # 4. åŸé€Ÿé‡æ’­
        part_idx += 1
        replay = work_path / f"part_{part_idx:03d}_replay.mp3"
        normalize_audio(str(highlight_orig), str(replay))
        parts.append(str(replay))
        # è®°å½•é‡æ’­æ—¶é—´è½´ï¼ˆåŒ…å«ç”Ÿè¯æ ‡è®°ï¼‰
        timeline.append(TimelineEntry(
            start_ms=enhanced_time_ms,
            text=ep.sentence_text,
            entry_type="replay",
            unfamiliar_words=ep.unfamiliar_words
        ))
        replay_duration = get_audio_duration_ms(str(replay))
        enhanced_time_ms += replay_duration
        print(f"  Part {part_idx}: replay at normal speed")

        current_pos = ep.end_ms

    # 5. æœ€åçš„åŸå§‹éŸ³é¢‘
    if current_pos < total_duration_ms:
        part_idx += 1
        after_file = work_path / f"part_{part_idx:03d}_after.mp3"
        cut_audio(source_audio, str(after_file), current_pos, total_duration_ms)
        parts.append(str(after_file))
        # è®°å½•æœ€åæ®µè½ä¸­çš„æ™®é€šå¥å­
        add_normal_sentences_to_timeline(current_pos, total_duration_ms, enhanced_time_ms)
        print(f"  Part {part_idx}: after segment [{current_pos/1000:.1f}s - {total_duration_ms/1000:.1f}s]")

    # æ‹¼æ¥æ‰€æœ‰ç‰‡æ®µ
    print(f"\nConcatenating {len(parts)} parts...")
    concat_audios(parts, str(output_file))

    # ç”Ÿæˆå­¦ä¹ ç¬”è®°
    print("Generating notes.txt...")
    notes_path = generate_notes_file(
        output_path,
        enhancement_points,
        source_url,
        int(max_duration_ms / 1000) if max_duration_ms else int(total_duration_ms / 1000)
    )

    # ç”Ÿæˆ LRC æ­Œè¯æ–‡ä»¶
    print("Generating enhanced.lrc...")
    lrc_path = generate_lrc_file(
        output_path,
        timeline,
        title=output_path.name
    )

    # è¾“å‡ºç»Ÿè®¡
    enhanced_duration = get_audio_duration_ms(str(output_file))
    print(f"\nEnhancement complete!")
    print(f"  Original duration: {total_duration_ms/1000:.1f}s")
    print(f"  Enhanced duration: {enhanced_duration/1000:.1f}s")
    print(f"  Timeline entries: {len(timeline)}")
    print(f"  Output directory: {output_dir}")
    print(f"  - enhanced.mp3")
    print(f"  - enhanced.lrc")
    print(f"  - notes.txt")

    return str(output_file), notes_path, lrc_path


def main():
    parser = argparse.ArgumentParser(description="è§†é¢‘è‹±è¯­å­¦ä¹ å¢å¼ºå·¥å…·")
    parser.add_argument("--audio", "-a", required=True, help="åŸå§‹éŸ³é¢‘æ–‡ä»¶")
    parser.add_argument("--asr", "-r", required=True, help="ASR JSON æ–‡ä»¶")

    # æ”¯æŒä¸¤ç§è¾“å‡ºæ–¹å¼
    output_group = parser.add_mutually_exclusive_group(required=True)
    output_group.add_argument("--output", "-o", help="è¾“å‡ºæ–‡ä»¶ï¼ˆæ—§ç‰ˆå…¼å®¹ï¼‰")
    output_group.add_argument("--output-dir", help="è¾“å‡ºç›®å½•ï¼ˆæ¨èï¼Œä¼šç”Ÿæˆ enhanced.mp3 å’Œ notes.txtï¼‰")

    parser.add_argument("--duration", "-d", type=int, help="åªå¤„ç†å‰ N ç§’")
    parser.add_argument("--target-wpm", type=int, default=100, help="ç›®æ ‡è¯­é€Ÿ WPMï¼ˆé»˜è®¤ 100ï¼‰")
    parser.add_argument("--url", "-u", help="åŸå§‹è§†é¢‘ URLï¼ˆå¯é€‰ï¼Œç”¨äº notes.txtï¼‰")
    parser.add_argument("--work-dir", "-w", help="å·¥ä½œç›®å½•ï¼ˆå­˜æ”¾ä¸´æ—¶æ–‡ä»¶ï¼‰")

    args = parser.parse_args()

    max_duration_ms = args.duration * 1000 if args.duration else None

    if args.output_dir:
        # æ–°ç‰ˆï¼šè¾“å‡ºåˆ°ç›®å½•
        enhance_audio(
            source_audio=args.audio,
            asr_file=args.asr,
            output_dir=args.output_dir,
            max_duration_ms=max_duration_ms,
            target_wpm=args.target_wpm,
            source_url=args.url or "",
            work_dir=args.work_dir,
        )
    else:
        # æ—§ç‰ˆå…¼å®¹ï¼šè¾“å‡ºåˆ°å•ä¸ªæ–‡ä»¶
        # åˆ›å»ºä¸´æ—¶ç›®å½•ï¼Œç„¶åç§»åŠ¨æ–‡ä»¶
        temp_dir = tempfile.mkdtemp()
        enhance_audio(
            source_audio=args.audio,
            asr_file=args.asr,
            output_dir=temp_dir,
            max_duration_ms=max_duration_ms,
            target_wpm=args.target_wpm,
            source_url=args.url or "",
            work_dir=args.work_dir,
        )
        # ç§»åŠ¨ enhanced.mp3 åˆ°æŒ‡å®šä½ç½®
        import shutil
        shutil.move(Path(temp_dir) / "enhanced.mp3", args.output)
        print(f"Output: {args.output}")


if __name__ == "__main__":
    main()
